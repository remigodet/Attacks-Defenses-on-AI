{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import tarfile\n",
    "import zipfile\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import six.moves.urllib as urllib\n",
    "# from io import StringIO\n",
    "# from  cStringIO import StringIO\n",
    "from io import BytesIO as StringIO\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils.visualization_utils import visualize_boxes_and_labels_on_image_array\n",
    "from object_detection.core import target_assigner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .. import utils\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path for images (for inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_PATH = \"This/is/my/path/to/images\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the target class for perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CLASS = 3 # target class: person 1 car 3 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OBJ_DETECT_API='~/data/'\n",
    "\n",
    "# MODEL_NAME = 'faster_rcnn_inception_v2_coco_2017_11_08'\n",
    "MODEL_NAME = 'ssd_inception_v2_coco_2018_01_28'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = './data/mscoco_label_map.pbtxt'\n",
    "\n",
    "NUM_CLASSES = 90"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the model from Tensorflow object detection lib : 'http://download.tensorflow.org/models/object_detection/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssd_inception_v2_coco_2018_01_28\n",
      "model.ckpt.index\n",
      "checkpoint\n",
      "pipeline.config\n",
      "model.ckpt.data-00000-of-00001\n",
      "model.ckpt.meta\n",
      "saved_model\n",
      "saved_model.pb\n",
      "variables\n",
      "frozen_inference_graph.pb\n",
      "<TarInfo 'ssd_inception_v2_coco_2018_01_28/frozen_inference_graph.pb' at 0x7f3eed5bc110> /usr/users/attaquesetdefensessurlia/godetrem\n"
     ]
    }
   ],
   "source": [
    "opener = urllib.request.URLopener()\n",
    "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "tar_file = tarfile.open(MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "  file_name = os.path.basename(\"./\"+file.name)\n",
    "  print(file_name)\n",
    "  if 'frozen_inference_graph.pb' in file_name:\n",
    "    print(file, os.getcwd())\n",
    "    tar_file.extract(file, os.getcwd())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions for loading and displaying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    img = PIL.Image.open(path)\n",
    "    img = np.array(img, dtype=np.uint8)\n",
    "    return img\n",
    "\n",
    "def showarray(a, fmt='png'):\n",
    "    a = np.uint8(a)\n",
    "    f = StringIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))\n",
    "\n",
    "\n",
    "\n",
    "# tests \n",
    "img = read_image('data/img_stop_sign.png')\n",
    "img3 = utils.read_image_resize('data/1.JPG', crop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the mask\n",
    "There are 2 masks, one for the red part of stop sign and one for the white part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# size of the perturbation we want to generate\n",
    "psize = 300\n",
    "WHITE_MASK = np.array(PIL.Image.fromarray(np.load('data/stop_white_mask.npy')).resize((psize, psize)))\n",
    "RED_MASK = np.array(PIL.Image.fromarray(np.load('data/stop_red_mask.npy')).resize((psize, psize))) # the one we modify"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inference_graph = tf.Graph()\n",
    "with inference_graph.as_default():\n",
    "    image_tensor = tf.placeholder(tf.float32, shape=(None, psize, psize, 3), name='image_tensor')\n",
    "    inference_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        inference_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(inference_graph_def, name='',\n",
    "                            input_map={'Preprocessor/map/TensorArrayStack/TensorArrayGatherV3:0':image_tensor})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining some plotting utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_detections(img, scores=None, bboxes=None, min_threshold=0):\n",
    "    if scores is None or bboxes is None:\n",
    "        \n",
    "        inference_sess = tf.Session(graph=inference_graph)\n",
    "        # print(inference_sess.graph_def)\n",
    "        tensors = [ inference_graph.get_tensor_by_name('detection_boxes:0'),\n",
    "                    inference_graph.get_tensor_by_name('detection_scores:0'),\n",
    "                    inference_graph.get_tensor_by_name('detection_classes:0'),\n",
    "                    inference_graph.get_tensor_by_name('num_detections:0'),\n",
    "                    # inference_graph.get_tensor_by_name('Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Reshape_4:0'),\n",
    "                    inference_graph.get_tensor_by_name('Postprocessor/convert_scores:0') ]\n",
    "\n",
    "        feed_dict = { inference_graph.get_tensor_by_name('image_tensor:0'): np.expand_dims(img, axis=0),inference_graph.get_tensor_by_name('image_tensor_1:0'): np.expand_dims(img, axis=0) }\n",
    "\n",
    "        nms_bboxes, nms_scores, nms_classes, num_detections, scores = inference_sess.run(tensors,\n",
    "                                                                                                 feed_dict)\n",
    "        \n",
    "        # bboxes = bboxes[0]\n",
    "        scores = scores[0]\n",
    "    \n",
    "    sorted_classes = np.argsort(scores[:, 1:], axis=1)\n",
    "    sorted_scores = scores[:, 1:].copy()\n",
    "    # sorted_bboxes = bboxes.copy()\n",
    "\n",
    "    for i, ordering in enumerate(sorted_classes):\n",
    "        sorted_scores[i, :] = scores[i, ordering+1]\n",
    "        # sorted_bboxes[i, :] = bboxes[i, ordering, :]\n",
    "\n",
    "    sorted_classes += 1\n",
    "    return sorted_scores, sorted_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.6778708e-05, 1.7732382e-05, 1.9699335e-05, ..., 2.8591156e-03,\n",
       "         2.9899180e-03, 6.3054860e-03],\n",
       "        [4.6044588e-05, 5.3942204e-05, 5.8412552e-05, ..., 1.8167198e-03,\n",
       "         2.0538867e-03, 2.2564232e-03],\n",
       "        [3.5256147e-05, 5.0067902e-05, 5.1081181e-05, ..., 9.8779798e-04,\n",
       "         1.9553900e-03, 3.9180517e-03],\n",
       "        ...,\n",
       "        [5.8114529e-05, 9.0032816e-05, 9.8526478e-05, ..., 1.1786819e-03,\n",
       "         1.2669265e-03, 1.7919540e-03],\n",
       "        [3.2067299e-05, 4.7683716e-05, 5.8978796e-05, ..., 8.0490112e-04,\n",
       "         1.5103817e-03, 2.1658242e-03],\n",
       "        [3.6358833e-06, 3.7252903e-06, 6.0498714e-06, ..., 1.4872253e-03,\n",
       "         5.5933297e-03, 1.3913006e-02]], dtype=float32),\n",
       " array([[34, 40, 23, ..., 62, 84,  1],\n",
       "        [16, 34, 38, ..., 78,  1, 62],\n",
       "        [40, 35, 34, ..., 79, 62,  1],\n",
       "        ...,\n",
       "        [18,  4, 52, ...,  7,  5, 78],\n",
       "        [18,  4, 88, ...,  9,  1, 82],\n",
       "        [ 4, 40, 18, ...,  7,  1, 82]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test detections on a given image from dataset\n",
    "path = f\"{YOUR_PATH}/shapeshifter_car_3.4°_0°_5m_f18mm_a0_185lux.JPG\"\n",
    "plot_detections(utils.read_image_resize(path,height=psize, width=psize, zoom=1.2), min_threshold=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute model performance on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_img_detection_data_from_path(img_path, image_folder_path, image_focal=None, zoom=None, scores=None, bboxes=None, min_threshold=0):\n",
    "    img = utils.get_img_cropped(image_folder_path, img_path, f=image_focal, max_crop = 0.4, min_crop=1, max_f=250, min_f=55, zoom=zoom, height=300, width=300)\n",
    "    if scores is None or bboxes is None:\n",
    "        \n",
    "        inference_sess = tf.Session(graph=inference_graph)\n",
    "        # print(inference_sess.graph_def)\n",
    "        tensors = [ inference_graph.get_tensor_by_name('detection_boxes:0'),\n",
    "                    inference_graph.get_tensor_by_name('detection_scores:0'),\n",
    "                    inference_graph.get_tensor_by_name('detection_classes:0'),\n",
    "                    inference_graph.get_tensor_by_name('num_detections:0'),\n",
    "                    # inference_graph.get_tensor_by_name('Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Reshape_4:0'),\n",
    "                    inference_graph.get_tensor_by_name('Postprocessor/convert_scores:0') ]\n",
    "\n",
    "        feed_dict = { inference_graph.get_tensor_by_name('image_tensor:0'): np.expand_dims(img, axis=0),inference_graph.get_tensor_by_name('image_tensor_1:0'): np.expand_dims(img, axis=0) }\n",
    "\n",
    "        nms_bboxes, nms_scores, nms_classes, num_detections, scores = inference_sess.run(tensors,\n",
    "                                                                                                 feed_dict)\n",
    "        \n",
    "        # bboxes = bboxes[0]\n",
    "        scores = scores[0]\n",
    "    \n",
    "    sorted_classes = np.argsort(scores[:, 1:], axis=1)\n",
    "    sorted_scores = scores[:, 1:].copy()\n",
    "    # sorted_bboxes = bboxes.copy()\n",
    "\n",
    "    for i, ordering in enumerate(sorted_classes):\n",
    "        sorted_scores[i, :] = scores[i, ordering+1]\n",
    "        # sorted_bboxes[i, :] = bboxes[i, ordering, :]\n",
    "\n",
    "    sorted_classes += 1\n",
    "    # return sorted_scores, sorted_classes\n",
    "    return sorted_classes, sorted_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic zoom ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get images inferences - approx 2 minutes\n",
    "image_folder_path = YOUR_PATH\n",
    "# load images names\n",
    "images_autocrop_df = utils.load_images_df(image_folder_path)\n",
    "# infer\n",
    "images_autocrop_df[[\"sorted_classes\" , \"sorted_scores\"]] = images_autocrop_df[[\"filename\", \"Focale\"]].apply(lambda x:pd.Series(get_img_detection_data_from_path(x[0], image_folder_path, image_focal=x[1])), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a fixed zoom ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get images inferences - approx 15 minutes\n",
    "images_dfs_zoom = {}\n",
    "image_folder_path = YOUR_PATH\n",
    "for zoom in [1,0.6,0.4]: # chosen fixed zooms \n",
    "    # load images \n",
    "    df = utils.load_images_df(image_folder_path)\n",
    "    # infer on a column\n",
    "    df[[\"sorted_classes\" , \"sorted_scores\", \"sorted_bboxes\"]] = df[[\"filename\", \"Focale\"]].apply(lambda x:pd.Series(get_img_detection_data_from_path(x[0], image_folder_path, zoom=zoom)), axis=1)\n",
    "    images_dfs_zoom[zoom] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(target_class,sorted_classes, sorted_scores,treshold=0.5, debug=None):\n",
    "    '''\n",
    "    The accuracy function : computes (in a range of classes, and over a treshold) the number of \n",
    "    correctly recognized classes over the total number of classes. \n",
    "    '''\n",
    "    if debug is not None:\n",
    "        print(\"=======================\")\n",
    "        print(debug)\n",
    "    temp = 0\n",
    "    targets = 0\n",
    "    # print(target_class)\n",
    "    # print(sorted_classes)\n",
    "    # print(sorted_scores)\n",
    "    for i in range(len(sorted_classes)):\n",
    "        for j in range(len(sorted_classes[i])):\n",
    "            if sorted_classes[i][j] in [13, 1 , 3]:\n",
    "                if sorted_scores[i][j]>=treshold:\n",
    "                    temp += 1\n",
    "                    print(sorted_classes[i][j], sorted_scores[i][j])\n",
    "                    if sorted_classes[i][j]==target_class:\n",
    "                        targets += 1\n",
    "    if temp==0:\n",
    "        return None\n",
    "    # print(targets, temp)\n",
    "    return targets  / temp      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy in forced zooms \n",
    "for zoom in images_dfs_zoom.keys():\n",
    "    image_df = images_dfs_zoom[zoom]\n",
    "    image_df[\"AccuracyPerson\"] = image_df.apply(lambda x: accuracy(1, x.sorted_classes, x.sorted_scores), axis=1)\n",
    "    image_df[\"AccuracyCar\"] = image_df.apply(lambda x: accuracy(3, x.sorted_classes, x.sorted_scores), axis=1)\n",
    "    image_df[\"AccuracyStopSign\"] = image_df.apply(lambda x: accuracy(13, x.sorted_classes, x.sorted_scores), axis=1)\n",
    "# Compute accuracy in autozoom\n",
    "images_autocrop_df[\"AccuracyPerson\"] = images_autocrop_df.apply(lambda x: accuracy(1, x.sorted_classes, x.sorted_scores), axis=1)\n",
    "images_autocrop_df[\"AccuracyCar\"] = images_autocrop_df.apply(lambda x: accuracy(3, x.sorted_classes, x.sorted_scores), axis=1)\n",
    "images_autocrop_df[\"AccuracyStopSign\"] = images_autocrop_df.apply(lambda x: accuracy(13, x.sorted_classes, x.sorted_scores), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insights\n",
    "# control_df[[\"filename\", \"Accuracy\"]]\n",
    "# person_df[[\"filename\", \"Accuracy\"]][person_df[\"Accuracy\"]<=1]\n",
    "# car_df[[\"filename\", \"sorted_classes\", \"Accuracy\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract values as list (pandas fun)\n",
    "images_autocrop_df[\"AccuracyPersonForcedZoom\"] = images_autocrop_df.apply(lambda x: [df[df[\"filename\"]==x[0]][\"AccuracyPerson\"].iloc[0] \n",
    "                                                                               for df in images_dfs_zoom.values()], axis=1)\n",
    "images_autocrop_df[\"AccuracyCarForcedZoom\"] = images_autocrop_df.apply(lambda x: [df[df[\"filename\"]==x[0]][\"AccuracyCar\"].iloc[0] \n",
    "                                                                               for df in images_dfs_zoom.values()], axis=1)\n",
    "images_autocrop_df[\"AccuracyStopSignForcedZoom\"] = images_autocrop_df.apply(lambda x: [df[df[\"filename\"]==x[0]][\"AccuracyStopSign\"].iloc[0] \n",
    "                                                                               for df in images_dfs_zoom.values()], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save as pickle (but this is sensible to modules version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_autocrop_df.to_pickle(\"dataframe_ssd\")\n",
    "# df = pd.read_pickle(\"dataframe\")\n",
    "# df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better to save as Excel: we can load it in windows, or use a lighter environment to do data analysis, see visualisation.py for this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"results.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the images "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is directly from the article's code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelContainer():\n",
    "    def __init__(self):\n",
    "        self.graph = tf.Graph()\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.patch_shape = (psize, psize, 3)\n",
    "        self.batch_size_ = 10\n",
    "        self._make_model_and_ops(None)\n",
    "\n",
    "    def get_patch(self):\n",
    "        patch = np.round((self._run(self.clipped_patch_)+1)*(255/2.0)).astype(np.uint8)\n",
    "        patch *= RED_MASK\n",
    "        patch[patch == 0] = 255\n",
    "        return patch\n",
    "\n",
    "    def assign_patch(self, new_patch):\n",
    "        self._run(self.assign_patch_, {self.patch_placeholder_: new_patch})\n",
    "\n",
    "    def reset_patch(self):\n",
    "        self.assign_patch(np.zeros(self.patch_shape))\n",
    "          \n",
    "    def train_step(self, images, patch_transforms, second_stage_cls_labels, learning_rate=1.0,\n",
    "                   dropout=None, rpn_nms_bboxes=None, rpn_nms_indices=None, patch_loss_weight=None):\n",
    "        if (rpn_nms_bboxes is None) or \\\n",
    "           (rpn_nms_indices is None):\n",
    "            rpn_nms_bboxes, rpn_nms_indices = self.inference_rpn(images, patch_transforms)\n",
    "\n",
    "        feed_dict = { self.image_input_: images,\n",
    "                      self.patch_transforms_: patch_transforms,\n",
    "                      self.second_stage_cls_labels_: second_stage_cls_labels,\n",
    "                      self.rpn_nms_bboxes_placeholder_: rpn_nms_bboxes,\n",
    "                      self.rpn_nms_indices_placeholder_: rpn_nms_indices,\n",
    "                      self.learning_rate_: learning_rate }\n",
    "        \n",
    "        if patch_loss_weight is not None:\n",
    "            feed_dict[self.patch_loss_weight_] = patch_loss_weight\n",
    "        \n",
    "        tensors = [ self.train_op_,\n",
    "                    self.loss_,\n",
    "                    self.second_stage_cls_loss_, \n",
    "                    self.patch_loss_]\n",
    "\n",
    "        train_op, loss, second_stage_cls_loss, patch_loss = self._run(tensors, feed_dict, dropout=dropout)\n",
    "\n",
    "        return loss, second_stage_cls_loss, patch_loss\n",
    "    \n",
    "    def inference_rpn(self, images, patch_transforms):\n",
    "        feed_dict = { self.image_input_: images,\n",
    "                      self.patch_transforms_: patch_transforms }\n",
    "        \n",
    "        tensors = [self.rpn_nms_bboxes_,\n",
    "                   self.rpn_nms_indices_ ]\n",
    "\n",
    "        rpn_nms_bboxes, rpn_nms_indices = self._run(tensors, feed_dict)\n",
    "        \n",
    "        return rpn_nms_bboxes, rpn_nms_indices\n",
    "\n",
    "    def inference(self, images, patch_transforms, rpn_nms_bboxes=None, rpn_nms_indices=None):\n",
    "        if (rpn_nms_bboxes is None) or \\\n",
    "           (rpn_nms_indices is None):\n",
    "            rpn_nms_bboxes, rpn_nms_indices = self.inference_rpn(images, patch_transforms)\n",
    "\n",
    "        feed_dict = { self.image_input_: images,\n",
    "                      self.patch_transforms_: patch_transforms,\n",
    "                      self.rpn_nms_bboxes_placeholder_: rpn_nms_bboxes,\n",
    "                      self.rpn_nms_indices_placeholder_: rpn_nms_indices }\n",
    "\n",
    "        tensors = [ self.patched_input_,\n",
    "                    self.second_stage_cls_scores_,\n",
    "                    self.second_stage_loc_bboxes_ ]\n",
    "\n",
    "        patched_imgs, second_stage_cls_scores, second_stage_loc_bboxes = self._run(tensors, feed_dict)\n",
    "        patched_imgs = patched_imgs.astype(np.uint8)\n",
    "\n",
    "        plot_detections(patched_imgs[0], scores=second_stage_cls_scores[0], bboxes=second_stage_loc_bboxes[0], min_threshold=0.2)\n",
    "        \n",
    "        return patched_imgs, second_stage_cls_scores, second_stage_loc_bboxes\n",
    "\n",
    "    def _run(self, target, feed_dict=None, dropout=None):\n",
    "        if feed_dict is None:\n",
    "            feed_dict = {}\n",
    "        \n",
    "        if dropout is not None:\n",
    "            feed_dict[self.dropout_] = dropout\n",
    "    \n",
    "        return self.sess.run(target, feed_dict=feed_dict)\n",
    "    \n",
    "    def _make_model_and_ops(self, patch_val):\n",
    "        start = time.time()\n",
    "        with self.sess.graph.as_default():\n",
    "            tf.set_random_seed(1234)\n",
    "            \n",
    "            # Tensors are post-fixed with an underscore!\n",
    "            self.image_input_ = tf.placeholder(tf.float32, shape=(None, psize, psize, 3), name='image_input')\n",
    "            self.patch_transforms_ = tf.placeholder(tf.float32, shape=(None, 8), name='patch_transforms')\n",
    "\n",
    "            patch_ = tf.get_variable('patch', self.patch_shape, dtype=tf.float32, initializer=tf.zeros_initializer)\n",
    "            self.patch_placeholder_ = tf.placeholder(dtype=tf.float32, shape=self.patch_shape, name='patch_placeholder')\n",
    "            self.assign_patch_ = tf.assign(patch_, self.patch_placeholder_)\n",
    "            self.clipped_patch_ = tf.tanh(patch_)\n",
    "\n",
    "            self.dropout_ = tf.placeholder_with_default(1.0, [], name='dropout')\n",
    "            patch_with_dropout_ = tf.nn.dropout(self.clipped_patch_, keep_prob=self.dropout_)\n",
    "            patched_input_ = tf.clip_by_value(self._random_overlay(self.image_input_, patch_with_dropout_), clip_value_min=-1.0, clip_value_max=1.0)\n",
    "            patched_input_ = tf.clip_by_value(tf.image.random_brightness(patched_input_, 10.0/255), -1.0, 1.0)\n",
    "            self.patched_input_ = tf.fake_quant_with_min_max_vars((patched_input_ + 1)*127.5, min=0, max=255)\n",
    "\n",
    "            # Create placeholders for NMS RPN inputs\n",
    "            self.rpn_nms_bboxes_placeholder_ = tf.placeholder(tf.float32, shape=(None, 4), name='rpn_nms_bboxes')\n",
    "            self.rpn_nms_indices_placeholder_ = tf.placeholder(tf.int32, shape=(None), name='rpn_nms_indices')\n",
    "\n",
    "            detection_graph_def = tf.GraphDef()\n",
    "            with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "                serialized_graph = fid.read()\n",
    "                detection_graph_def.ParseFromString(serialized_graph)\n",
    "                tf.import_graph_def(detection_graph_def, name='detection',\n",
    "                                    input_map={\n",
    "                                               'Preprocessor/map/TensorArrayStack/TensorArrayGatherV3:0':self.patched_input_,\n",
    "                                               'Reshape_7:0':self.rpn_nms_bboxes_placeholder_,\n",
    "                                               'Reshape_8:0':self.rpn_nms_indices_placeholder_,\n",
    "                                              })\n",
    "\n",
    "            # Recreate tensors we just replaced in the input_map\n",
    "            self.rpn_nms_bboxes_ = tf.reshape(self.graph.get_tensor_by_name('detection/Reshape_6:0'), self.graph.get_tensor_by_name('detection/stack_3:0'), name='detection/Reshape_7')\n",
    "            self.rpn_nms_indices_ = tf.reshape(self.graph.get_tensor_by_name('detection/mul_1:0'), self.graph.get_tensor_by_name('detection/Reshape_8/shape:0'), name='detection/Reshape_8') \n",
    "\n",
    "            # Patch Loss\n",
    "            self.patch_loss_ = tf.nn.l2_loss(RED_MASK*(self.clipped_patch_ - np.tile(np.array([ 1.0, -0.9, -1]), (psize, psize, 1))))\n",
    "            self.patch_loss_weight_ = tf.placeholder_with_default(1.0, [], 'patch_loss_weight')\n",
    "\n",
    "            # Second-stage Class Loss\n",
    "            self.second_stage_cls_scores_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/convert_scores:0')\n",
    "            second_stage_cls_logits_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/scale_logits:0')\n",
    "            self.second_stage_cls_labels_ = tf.placeholder(tf.float32, shape=second_stage_cls_logits_.shape, name='second_stage_cls_labels')\n",
    "            second_stage_cls_losses_ = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.reshape(self.second_stage_cls_labels_, (-1, self.second_stage_cls_labels_.shape[2])),\n",
    "                                                                                      logits=tf.reshape(second_stage_cls_logits_, (-1, second_stage_cls_logits_.shape[2]))) \n",
    "            second_stage_cls_losses_ = tf.reshape(second_stage_cls_losses_, (-1, self.second_stage_cls_labels_.shape[1]))\n",
    "            second_stage_cls_losses_ = tf.divide(second_stage_cls_losses_, tf.to_float(self.second_stage_cls_labels_.shape[1]))\n",
    "            self.second_stage_cls_loss_ = tf.reduce_sum(second_stage_cls_losses_)\n",
    "           \n",
    "            # Second-stage bounding boxes\n",
    "            self.second_stage_loc_bboxes_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/Reshape_4:0')\n",
    "    \n",
    "            # Sum of weighted losses\n",
    "            self.loss_ = self.patch_loss_*self.patch_loss_weight_ + (self.second_stage_cls_loss_)\n",
    "\n",
    "            # Train our attack by only training on the patch variable\n",
    "            self.learning_rate_ = tf.placeholder(tf.float32)\n",
    "            self.train_op_ = tf.train.GradientDescentOptimizer(self.learning_rate_).minimize(self.loss_, var_list=[patch_])\n",
    "            \n",
    "            if patch_val is not None:\n",
    "                self.assign_patch(patch_val)\n",
    "            else:\n",
    "                self.reset_patch()\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Finished loading the model, took {:.0f}s\".format(elapsed))\n",
    "    \n",
    "\n",
    "    def _random_overlay(self, imgs, patch):    \n",
    "        red_mask = RED_MASK.astype(np.float32)\n",
    "        white_mask = WHITE_MASK.astype(np.float32)\n",
    "        \n",
    "        red_mask = tf.stack([red_mask] * self.batch_size_)\n",
    "        white_mask = tf.stack([white_mask] * self.batch_size_)\n",
    "        padded_patch = tf.stack([patch] * self.batch_size_)\n",
    "        \n",
    "        white = tf.ones_like(red_mask) * 0.95\n",
    "              \n",
    "        red_mask = tf.contrib.image.transform(red_mask, self.patch_transforms_, 'BILINEAR')\n",
    "        white_mask = tf.contrib.image.transform(white_mask, self.patch_transforms_, 'BILINEAR')\n",
    "        padded_patch = tf.contrib.image.transform(padded_patch, self.patch_transforms_, 'BILINEAR')\n",
    "\n",
    "        inverted_mask = (1 - red_mask - white_mask)\n",
    "\n",
    "        return white * white_mask + imgs * inverted_mask + padded_patch * red_mask\n",
    "    \n",
    "\n",
    "    def _transform_vector(self, width, x_shift, y_shift, im_scale, rot_in_degrees):\n",
    "        \"\"\"\n",
    "        If one row of transforms is [a0, a1, a2, b0, b1, b2, c0, c1], \n",
    "        then it maps the output point (x, y) to a transformed input point \n",
    "        (x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k), \n",
    "        where k = c0 x + c1 y + 1. \n",
    "        The transforms are inverted compared to the transform mapping input points to output points.\n",
    "        \"\"\"\n",
    "\n",
    "        rot = float(rot_in_degrees) / 90. * (math.pi/2)\n",
    "\n",
    "        # Standard rotation matrix\n",
    "        # (use negative rot because tf.contrib.image.transform will do the inverse)\n",
    "        rot_matrix = np.array(\n",
    "            [[math.cos(-rot), -math.sin(-rot)],\n",
    "            [math.sin(-rot), math.cos(-rot)]]\n",
    "        )\n",
    "\n",
    "        # Scale it\n",
    "        # (use inverse scale because tf.contrib.image.transform will do the inverse)\n",
    "        inv_scale = 1. / im_scale \n",
    "        xform_matrix = rot_matrix * inv_scale\n",
    "        a0, a1 = xform_matrix[0]\n",
    "        b0, b1 = xform_matrix[1]\n",
    "\n",
    "        # At this point, the image will have been rotated around the top left corner,\n",
    "        # rather than around the center of the image. \n",
    "        #\n",
    "        # To fix this, we will see where the center of the image got sent by our transform,\n",
    "        # and then undo that as part of the translation we apply.\n",
    "        x_origin = float(width) / 2\n",
    "        y_origin = float(width) / 2\n",
    "\n",
    "        x_origin_shifted, y_origin_shifted = np.matmul(\n",
    "            xform_matrix,\n",
    "            np.array([x_origin, y_origin]),\n",
    "        )\n",
    "\n",
    "        x_origin_delta = x_origin - x_origin_shifted\n",
    "        y_origin_delta = y_origin - y_origin_shifted\n",
    "\n",
    "        # Combine our desired shifts with the rotation-induced undesirable shift\n",
    "        a2 = x_origin_delta - (x_shift/(2*im_scale))\n",
    "        b2 = y_origin_delta - (y_shift/(2*im_scale))\n",
    "\n",
    "        # Return these values in the order that tf.contrib.image.transform expects\n",
    "        return np.array([a0, a1, a2, b0, b1, b2, 0, 0]).astype(np.float32)\n",
    "\n",
    "    def generate_random_transformation(self, scale_min=0.2, scale_max=0.6, width=psize, max_rotation=20):\n",
    "        im_scale = np.random.uniform(low=scale_min, high=scale_max)\n",
    "\n",
    "        padding_after_scaling = (1-im_scale) * width\n",
    "        x_delta = np.random.uniform(-padding_after_scaling, padding_after_scaling)\n",
    "        y_delta = np.random.uniform(-padding_after_scaling, padding_after_scaling)\n",
    "\n",
    "        rot = np.random.uniform(-max_rotation, max_rotation)\n",
    "\n",
    "        return self._transform_vector(width, \n",
    "                                      x_shift=x_delta,\n",
    "                                      y_shift=y_delta,\n",
    "                                      im_scale=im_scale, \n",
    "                                      rot_in_degrees=rot)    \n",
    "\n",
    "model = ModelContainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     240
    ],
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ModelContainer():\n",
    "    def __init__(self):\n",
    "        self.graph = tf.Graph()\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.patch_shape = (psize, psize, 3)\n",
    "        self.batch_size_ = 10\n",
    "        self._make_model_and_ops(None)\n",
    "\n",
    "    def get_patch(self):\n",
    "        patch = np.round((self._run(self.clipped_patch_)+1)*(255/2.0)).astype(np.uint8)\n",
    "        patch *= RED_MASK\n",
    "        patch[patch == 0] = 255\n",
    "        return patch\n",
    "\n",
    "    def assign_patch(self, new_patch):\n",
    "        self._run(self.assign_patch_, {self.patch_placeholder_: new_patch})\n",
    "\n",
    "    def reset_patch(self):\n",
    "        self.assign_patch(np.zeros(self.patch_shape))\n",
    "          \n",
    "    def train_step(self, images, patch_transforms, second_stage_cls_labels, learning_rate=1.0,\n",
    "                   dropout=None, rpn_nms_bboxes=None, rpn_nms_indices=None, patch_loss_weight=None):\n",
    "        if (rpn_nms_bboxes is None) or \\\n",
    "           (rpn_nms_indices is None):\n",
    "            rpn_nms_bboxes, rpn_nms_indices = self.inference_rpn(images, patch_transforms)\n",
    "\n",
    "        feed_dict = { self.image_input_: images,\n",
    "                      self.patch_transforms_: patch_transforms,\n",
    "                      self.second_stage_cls_labels_: second_stage_cls_labels,\n",
    "                      self.rpn_nms_bboxes_placeholder_: rpn_nms_bboxes,\n",
    "                      self.rpn_nms_indices_placeholder_: rpn_nms_indices,\n",
    "                      self.learning_rate_: learning_rate }\n",
    "        \n",
    "        if patch_loss_weight is not None:\n",
    "            feed_dict[self.patch_loss_weight_] = patch_loss_weight\n",
    "        \n",
    "        tensors = [ self.train_op_,\n",
    "                    self.loss_,\n",
    "                    self.second_stage_cls_loss_, \n",
    "                    self.patch_loss_]\n",
    "\n",
    "        train_op, loss, second_stage_cls_loss, patch_loss = self._run(tensors, feed_dict, dropout=dropout)\n",
    "\n",
    "        return loss, second_stage_cls_loss, patch_loss\n",
    "    \n",
    "    def inference_rpn(self, images, patch_transforms):\n",
    "        feed_dict = { self.image_input_: images,\n",
    "                      self.patch_transforms_: patch_transforms }\n",
    "        \n",
    "        tensors = [self.rpn_nms_bboxes_,\n",
    "                   self.rpn_nms_indices_ ]\n",
    "\n",
    "        rpn_nms_bboxes, rpn_nms_indices = self._run(tensors, feed_dict)\n",
    "        \n",
    "        return rpn_nms_bboxes, rpn_nms_indices\n",
    "\n",
    "    def inference(self, images, patch_transforms, rpn_nms_bboxes=None, rpn_nms_indices=None):\n",
    "        if (rpn_nms_bboxes is None) or \\\n",
    "           (rpn_nms_indices is None):\n",
    "            rpn_nms_bboxes, rpn_nms_indices = self.inference_rpn(images, patch_transforms)\n",
    "\n",
    "        feed_dict = { self.image_input_: images,\n",
    "                      self.patch_transforms_: patch_transforms,\n",
    "                      self.rpn_nms_bboxes_placeholder_: rpn_nms_bboxes,\n",
    "                      self.rpn_nms_indices_placeholder_: rpn_nms_indices }\n",
    "\n",
    "        tensors = [ self.patched_input_,\n",
    "                    self.second_stage_cls_scores_,\n",
    "                    self.second_stage_loc_bboxes_ ]\n",
    "\n",
    "        patched_imgs, second_stage_cls_scores, second_stage_loc_bboxes = self._run(tensors, feed_dict)\n",
    "        patched_imgs = patched_imgs.astype(np.uint8)\n",
    "\n",
    "        plot_detections(patched_imgs[0], scores=second_stage_cls_scores[0], bboxes=second_stage_loc_bboxes[0], min_threshold=0.2)\n",
    "        \n",
    "        return patched_imgs, second_stage_cls_scores, second_stage_loc_bboxes\n",
    "\n",
    "    def _run(self, target, feed_dict=None, dropout=None):\n",
    "        if feed_dict is None:\n",
    "            feed_dict = {}\n",
    "        \n",
    "        if dropout is not None:\n",
    "            feed_dict[self.dropout_] = dropout\n",
    "    \n",
    "        return self.sess.run(target, feed_dict=feed_dict)\n",
    "    \n",
    "    def _make_model_and_ops(self, patch_val):\n",
    "        start = time.time()\n",
    "        with self.sess.graph.as_default():\n",
    "            tf.set_random_seed(1234)\n",
    "            \n",
    "            # Tensors are post-fixed with an underscore!\n",
    "            self.image_input_ = tf.placeholder(tf.float32, shape=(None, psize, psize, 3), name='image_input')\n",
    "            self.patch_transforms_ = tf.placeholder(tf.float32, shape=(None, 8), name='patch_transforms')\n",
    "\n",
    "            patch_ = tf.get_variable('patch', self.patch_shape, dtype=tf.float32, initializer=tf.zeros_initializer)\n",
    "            self.patch_placeholder_ = tf.placeholder(dtype=tf.float32, shape=self.patch_shape, name='patch_placeholder')\n",
    "            self.assign_patch_ = tf.assign(patch_, self.patch_placeholder_)\n",
    "            self.clipped_patch_ = tf.tanh(patch_)\n",
    "\n",
    "            self.dropout_ = tf.placeholder_with_default(1.0, [], name='dropout')\n",
    "            patch_with_dropout_ = tf.nn.dropout(self.clipped_patch_, keep_prob=self.dropout_)\n",
    "            patched_input_ = tf.clip_by_value(self._random_overlay(self.image_input_, patch_with_dropout_), clip_value_min=-1.0, clip_value_max=1.0)\n",
    "            patched_input_ = tf.clip_by_value(tf.image.random_brightness(patched_input_, 10.0/255), -1.0, 1.0)\n",
    "            self.patched_input_ = tf.fake_quant_with_min_max_vars((patched_input_ + 1)*127.5, min=0, max=255)\n",
    "\n",
    "            # Create placeholders for NMS RPN inputs\n",
    "            self.rpn_nms_bboxes_placeholder_ = tf.placeholder(tf.float32, shape=(None, 4), name='rpn_nms_bboxes')\n",
    "            self.rpn_nms_indices_placeholder_ = tf.placeholder(tf.int32, shape=(None), name='rpn_nms_indices')\n",
    "\n",
    "            detection_graph_def = tf.GraphDef()\n",
    "            with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "                serialized_graph = fid.read()\n",
    "                detection_graph_def.ParseFromString(serialized_graph)\n",
    "                tf.import_graph_def(detection_graph_def, name='detection',\n",
    "                                    input_map={\n",
    "                                               'Preprocessor/map/TensorArrayStack/TensorArrayGatherV3:0':self.patched_input_,\n",
    "                                               'Reshape_7:0':self.rpn_nms_bboxes_placeholder_,\n",
    "                                               'Reshape_8:0':self.rpn_nms_indices_placeholder_,\n",
    "                                              })\n",
    "\n",
    "            # Recreate tensors we just replaced in the input_map\n",
    "            self.rpn_nms_bboxes_ = tf.reshape(self.graph.get_tensor_by_name('detection/Reshape_6:0'), self.graph.get_tensor_by_name('detection/stack_3:0'), name='detection/Reshape_7')\n",
    "            self.rpn_nms_indices_ = tf.reshape(self.graph.get_tensor_by_name('detection/mul_1:0'), self.graph.get_tensor_by_name('detection/Reshape_8/shape:0'), name='detection/Reshape_8') \n",
    "\n",
    "            # Patch Loss\n",
    "            self.patch_loss_ = tf.nn.l2_loss(RED_MASK*(self.clipped_patch_ - np.tile(np.array([ 1.0, -0.9, -1]), (psize, psize, 1))))\n",
    "            self.patch_loss_weight_ = tf.placeholder_with_default(1.0, [], 'patch_loss_weight')\n",
    "\n",
    "            # Second-stage Class Loss\n",
    "            self.second_stage_cls_scores_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/convert_scores:0')\n",
    "            second_stage_cls_logits_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/scale_logits:0')\n",
    "            self.second_stage_cls_labels_ = tf.placeholder(tf.float32, shape=second_stage_cls_logits_.shape, name='second_stage_cls_labels')\n",
    "            second_stage_cls_losses_ = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.reshape(self.second_stage_cls_labels_, (-1, self.second_stage_cls_labels_.shape[2])),\n",
    "                                                                                      logits=tf.reshape(second_stage_cls_logits_, (-1, second_stage_cls_logits_.shape[2]))) \n",
    "            second_stage_cls_losses_ = tf.reshape(second_stage_cls_losses_, (-1, self.second_stage_cls_labels_.shape[1]))\n",
    "            second_stage_cls_losses_ = tf.divide(second_stage_cls_losses_, tf.to_float(self.second_stage_cls_labels_.shape[1]))\n",
    "            self.second_stage_cls_loss_ = tf.reduce_sum(second_stage_cls_losses_)\n",
    "           \n",
    "            # Second-stage bounding boxes\n",
    "            self.second_stage_loc_bboxes_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/Reshape_4:0')\n",
    "    \n",
    "            # Sum of weighted losses\n",
    "            self.loss_ = self.patch_loss_*self.patch_loss_weight_ + (self.second_stage_cls_loss_)\n",
    "\n",
    "            # Train our attack by only training on the patch variable\n",
    "            self.learning_rate_ = tf.placeholder(tf.float32)\n",
    "            self.train_op_ = tf.train.GradientDescentOptimizer(self.learning_rate_).minimize(self.loss_, var_list=[patch_])\n",
    "            \n",
    "            if patch_val is not None:\n",
    "                self.assign_patch(patch_val)\n",
    "            else:\n",
    "                self.reset_patch()\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Finished loading the model, took {:.0f}s\".format(elapsed))\n",
    "    \n",
    "\n",
    "    def _random_overlay(self, imgs, patch):    \n",
    "        red_mask = RED_MASK.astype(np.float32)\n",
    "        white_mask = WHITE_MASK.astype(np.float32)\n",
    "        \n",
    "        red_mask = tf.stack([red_mask] * self.batch_size_)\n",
    "        white_mask = tf.stack([white_mask] * self.batch_size_)\n",
    "        padded_patch = tf.stack([patch] * self.batch_size_)\n",
    "        \n",
    "        white = tf.ones_like(red_mask) * 0.95\n",
    "              \n",
    "        red_mask = tf.contrib.image.transform(red_mask, self.patch_transforms_, 'BILINEAR')\n",
    "        white_mask = tf.contrib.image.transform(white_mask, self.patch_transforms_, 'BILINEAR')\n",
    "        padded_patch = tf.contrib.image.transform(padded_patch, self.patch_transforms_, 'BILINEAR')\n",
    "\n",
    "        inverted_mask = (1 - red_mask - white_mask)\n",
    "\n",
    "        return white * white_mask + imgs * inverted_mask + padded_patch * red_mask\n",
    "    \n",
    "\n",
    "    def _transform_vector(self, width, x_shift, y_shift, im_scale, rot_in_degrees):\n",
    "        \"\"\"\n",
    "        If one row of transforms is [a0, a1, a2, b0, b1, b2, c0, c1], \n",
    "        then it maps the output point (x, y) to a transformed input point \n",
    "        (x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k), \n",
    "        where k = c0 x + c1 y + 1. \n",
    "        The transforms are inverted compared to the transform mapping input points to output points.\n",
    "        \"\"\"\n",
    "\n",
    "        rot = float(rot_in_degrees) / 90. * (math.pi/2)\n",
    "\n",
    "        # Standard rotation matrix\n",
    "        # (use negative rot because tf.contrib.image.transform will do the inverse)\n",
    "        rot_matrix = np.array(\n",
    "            [[math.cos(-rot), -math.sin(-rot)],\n",
    "            [math.sin(-rot), math.cos(-rot)]]\n",
    "        )\n",
    "\n",
    "        # Scale it\n",
    "        # (use inverse scale because tf.contrib.image.transform will do the inverse)\n",
    "        inv_scale = 1. / im_scale \n",
    "        xform_matrix = rot_matrix * inv_scale\n",
    "        a0, a1 = xform_matrix[0]\n",
    "        b0, b1 = xform_matrix[1]\n",
    "\n",
    "        # At this point, the image will have been rotated around the top left corner,\n",
    "        # rather than around the center of the image. \n",
    "        #\n",
    "        # To fix this, we will see where the center of the image got sent by our transform,\n",
    "        # and then undo that as part of the translation we apply.\n",
    "        x_origin = float(width) / 2\n",
    "        y_origin = float(width) / 2\n",
    "\n",
    "        x_origin_shifted, y_origin_shifted = np.matmul(\n",
    "            xform_matrix,\n",
    "            np.array([x_origin, y_origin]),\n",
    "        )\n",
    "\n",
    "        x_origin_delta = x_origin - x_origin_shifted\n",
    "        y_origin_delta = y_origin - y_origin_shifted\n",
    "\n",
    "        # Combine our desired shifts with the rotation-induced undesirable shift\n",
    "        a2 = x_origin_delta - (x_shift/(2*im_scale))\n",
    "        b2 = y_origin_delta - (y_shift/(2*im_scale))\n",
    "\n",
    "        # Return these values in the order that tf.contrib.image.transform expects\n",
    "        return np.array([a0, a1, a2, b0, b1, b2, 0, 0]).astype(np.float32)\n",
    "\n",
    "    def generate_random_transformation(self, scale_min=0.2, scale_max=0.6, width=psize, max_rotation=20):\n",
    "        im_scale = np.random.uniform(low=scale_min, high=scale_max)\n",
    "\n",
    "        padding_after_scaling = (1-im_scale) * width\n",
    "        x_delta = np.random.uniform(-padding_after_scaling, padding_after_scaling)\n",
    "        y_delta = np.random.uniform(-padding_after_scaling, padding_after_scaling)\n",
    "\n",
    "        rot = np.random.uniform(-max_rotation, max_rotation)\n",
    "\n",
    "        return self._transform_vector(width, \n",
    "                                      x_shift=x_delta,\n",
    "                                      y_shift=y_delta,\n",
    "                                      im_scale=im_scale, \n",
    "                                      rot_in_degrees=rot)    \n",
    "\n",
    "model = ModelContainer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_target_labels(scores, from_class, to_class):\n",
    "    target_labels = np.zeros_like(scores)\n",
    "    classes = np.argmax(scores[:, :, 1:], axis=2)+1\n",
    "\n",
    "    for i, _ in enumerate(classes):\n",
    "        for j, cls in enumerate(classes[i]):\n",
    "            cls = to_class # Just perturb all of them!\n",
    "            target_labels[i, j, cls] = 1\n",
    "\n",
    "    return target_labels\n",
    "\n",
    "# use half white images and half random noise images as the background images for the optimization\n",
    "# print(model.batch_size_/2, type(model.batch_size_/2), type(psize)) # debug\n",
    "white_imgs = np.ones((int(model.batch_size_ / 2), psize, psize, 3))\n",
    "noisy_imgs = np.random.rand(int(model.batch_size_ / 2), psize, psize, 3) * 2 - 1.0\n",
    "bg_imgs = np.concatenate([ noisy_imgs, white_imgs])\n",
    "\n",
    "patch_transformations = np.zeros((model.batch_size_, 8))\n",
    "for i in range(patch_transformations.shape[0]):\n",
    "    patch_transformations[i, :] = model.generate_random_transformation()\n",
    "\n",
    "_, scores, _ = model.inference(bg_imgs, patch_transformations)\n",
    "target_labels = create_target_labels(scores, 13, TARGET_CLASS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.reset_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "for i in range(251): #251\n",
    "    try:\n",
    "        # Generate random transformations\n",
    "        for j in range(patch_transformations.shape[0]):\n",
    "            patch_transformations[j, :] = model.generate_random_transformation(scale_min=0.05, scale_max=0.3)\n",
    "\n",
    "        # Update patch according to changed labels\n",
    "        loss, second_stage_cls_loss, patch_loss = model.train_step(bg_imgs,\n",
    "                                                                   patch_transformations,\n",
    "                                                                   target_labels,\n",
    "                                                                   learning_rate=1.0,\n",
    "                                                                   patch_loss_weight=0.005)\n",
    "\n",
    "        if (i % 10) == 0:\n",
    "            t1 = time.time()\n",
    "            print ('iter {} total loss: {} target loss: {} patch loss: {}'.format(i, loss, second_stage_cls_loss, patch_loss))\n",
    "            print(\"time elapsed: \", t1-t0)\n",
    "            t0 = t1\n",
    "            model.inference(bg_imgs, patch_transformations)\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print ('iter {} total loss: {} target loss: {} patch loss: {}'.format(i, loss, second_stage_cls_loss, patch_loss))\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patch = model.get_patch()\n",
    "showarray(PIL.Image.fromarray(patch))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patch = model.get_patch()\n",
    "for j in range(patch_transformations.shape[0]):\n",
    "    patch_transformations[j, :] = model.generate_random_transformation(scale_min=0.05, scale_max=0.3)\n",
    "_ = model.inference(bg_imgs, patch_transformations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PIL.Image.fromarray(patch).save('perturbation.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
